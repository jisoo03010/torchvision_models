# -*- coding: utf-8 -*-
"""densenet161

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dCsHpnFnKG0Ri4T0irFkXTZjPDl0uJ43
"""

from google.colab import drive

drive.mount('/content/drive')

!unzip /content/drive/MyDrive/criminal_city/dataset2.zip -d /content/sample_data/

pip install split-folders

import splitfolders

splitfolders.ratio('/content/sample_data/dataset2/', output="/content/sample_data/", seed=1337, ratio=(0.8, 0.2))

!nvidia-smi

import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn
import torchvision
import splitfolders

from torchvision import transforms, utils
from skimage import transform
import torchvision.datasets as datasets
import os
import numpy as np

import matplotlib.pyplot as plt

import pandas as pd

num_classes = 3
batch_size = 32
num_workers = 0 # 멀티 프로세싱과 관련된 파라미터
lr = 0.004

data_dir = '/content/sample_data/'
train_dir = os.path.join(data_dir, 'train')
test_dir = os.path.join(data_dir, 'val')
print(train_dir)

data_transforms = {
    'train': transforms.Compose([
        transforms.Resize((230, 230)),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(),  # 확률적으로 이미지를 수평으로 뒤집음
        # train 데이터를 랜덤으로 변형시켜서 학습 (overfitting 방지)
        transforms.ToTensor(), # numpy 이미지에서 torch (배열) 이미지로 변경
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize((230, 230)),
        # 이미지 사이즈 변경 (전처리)
        transforms.RandomCrop(224),
         # train으로 훈련시킨 모델의 정확도를 판별하기 위해 변형 X
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

# 이미지 불러오기
datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                    data_transforms[x])
                  for x in ['train', 'val']}


                  #
                  #

dataloaders = {x: torch.utils.data.DataLoader(datasets[x], # train
                                              batch_size=batch_size, 
                                              # shuffle=False : 출력값의 숫자를 무작위 추출이 아닌 순차적 추출
                                              # shuflle=True : 출력값의 숫자를 순차적이 아닌 무작위로 값 출력
                                              shuffle=True,
                                              num_workers=num_workers, # cpu작업을 몇 개의 코어를 사용해서 진행할지에 대한 설정 
                                              pin_memory=True,
                                              drop_last=False)
               for x in ['train', 'val']}

samples, labels = iter(dataloaders['val']).next()
classes = {0: 'eum', 1: 'ma', 2: 'son'}
fig = plt.figure(figsize=(8, 16))       # (8, 16) 사이즈의 영역
for i in range(16):
    a = fig.add_subplot(4, 4, i+1)      # fig 내부에 4행 4열을 만들고 하나씩 선택
    a.set_title(classes[labels[i].item()])  # 이미지에 classes로 제목
    a.axis('off')                   #    0        1      2
    # sample[i].numpy: (3, 224, 224): (channel, height, width) -> (height, width, channel)
    a.imshow(np.transpose(samples[i].numpy(), (1, 2, 0)))
plt.subplots_adjust(top=0.5)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") # GPU 여부에 따라서 사용할 디바이스 종류 자동 지정

import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import CIFAR10
from torchvision.transforms import ToTensor
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
import torchvision.models as models
from torchvision import transforms
from torchvision.utils import save_image
import torch.optim as optim

model =  models.densenet161(pretrained=False, num_classes=num_classes)
model.to(device) # 모델을 디바이스 안에 불러옴
loss_fn = nn.CrossEntropyLoss() # LOSS함수  
optimizer = torch.optim.SGD(model.parameters(), lr=lr)


train_loss = []
test_loss = [] 
train_accu = []
test_accu = []

epochs = 100
import time
for t in range(epochs):
  start_time = time.time()
  print(f"Epoch {t+1}\n-------------------------------")    
  loss_eq = 0
  for x, y in dataloaders['train']:
    x, y = x.to(device), y.to(device)
    pred = model(x) # 모델 입력한 값
    loss = loss_fn(pred, y) 

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
  # item : tensor에 저장된 값만 가져옴
    loss_eq += loss.item() 
    #current= batch * len(x)
  print(f"loss: {(loss):>7f}")

  train_loss.append(loss)
  #train_accu.append(current)        

  model.eval()

  total = 0
  loss_eq = 0
  correct =  0

  #with 구분 : close()닫는 코드를 생성하지 않아도 자동으로 닫아준다.
  with torch.no_grad(): # gradient(기울기) 계산 context를 비활성화 해주는 역할함 . interence나 va
    for x, y in dataloaders['val']:
      x = x.to(device)
      y = y.to(device)

      pred = model(x)
      total += len(y) #221
      loss = loss_fn(pred, y)

      loss_eq += loss.item()
      correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    print("correct =>>", correct)  
    print("total =>>", total)  
    print(f"Test Error: Accuracy: { 100 * correct / total:>0.1f}%, Avg loss: {(loss_eq / len(dataloaders['val'])):>8f}")  
    finish_time = time.time()
    Epoch_time = finish_time - start_time
    print(f"Epoch [{t+1}/{epochs}]: {int(Epoch_time)} seconds \n")

  test_loss.append(loss_eq / len(dataloaders['val']))   
  test_accu.append(100 *correct / total)

import matplotlib.pyplot as plt 
plt.plot(torch.as_tensor(train_loss), label='train loss')
plt.plot(torch.as_tensor(test_loss), label='val loss')
plt.legend()

plt.plot([t for t in torch.as_tensor(train_accu)], label='train accu')
plt.plot([t for t in torch.as_tensor(test_accu)], label='val accu')
plt.legend()





